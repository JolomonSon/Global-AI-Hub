{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYmYuGSWFs3-"
   },
   "source": [
    "# Building a Recurrent Neural Network\n",
    "\n",
    "## Sentiment Analysis\n",
    "In this project, we will build a Long Short-term Memory (LSTM) neural network to solve a binary sentiment analysis problem.\n",
    "\n",
    "For this, we'll use the â€œIMDB Movie Review Dataset\" available on Keras. It includes 50000 highly polarized movie reviews categorized as positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQQ7xy4lzfsw"
   },
   "source": [
    "## Importing the required libraries\n",
    "We'll start with importing required libraries.\n",
    "\n",
    "ðŸ“Œ Use the keyword \"import\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b__mue-XGPZ9"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_docstring' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\Mary Solomon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Import NumPy and Matplotlib\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\__init__.py:113\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_docstring' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\Mary Solomon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import NumPy and Matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0b5YzGHP3qs"
   },
   "source": [
    "## Dataset\n",
    "Let's download the IMDB dataset which is included in Keras, and assign it to the corresponding variables *X_train*, *y_train*, *X_test*, and *y_test*. We want to include the most frequently used 10000 words, so we specify 10000 for the num_words parameter.\n",
    "\n",
    "ðŸ“Œ Use the datasets.imdb.load_data() function of the Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WLgLQxGGDz8"
   },
   "outputs": [],
   "source": [
    "# Download the IMDB dataset included in Keras\n",
    "# Set the parameter num_words to 10000\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUPnNCgC0mHm"
   },
   "source": [
    "Before we move on, we can print a single sample to see what the data looks like.\n",
    "\n",
    "ðŸ“Œ Use the print() function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1spB5eY9xh-B"
   },
   "outputs": [],
   "source": [
    "# Print a sample\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKkhznIa8hIw"
   },
   "source": [
    "Then, we print the the number of samples in the X_train and X_test datasets to see how the dataset is distributed.\n",
    "\n",
    "ðŸ“Œ Use f-strings for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skzb2oTCdV-c"
   },
   "outputs": [],
   "source": [
    "# Print the number of samples\n",
    "print(\"X_train: {}\".format(len(X_train)))\n",
    "print(\"X_test: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF6kV-EsP5vS"
   },
   "source": [
    "# Preprocessing\n",
    "### Concatenate\n",
    "\n",
    "To split the dataset with 80-10-10 ratio, we'll first concatenate train and test datasets to create one big dataset.\n",
    "\n",
    "ðŸ“Œ Use contenate() function of the NumPy library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Whj2C-SlKv2E"
   },
   "outputs": [],
   "source": [
    "# Concatenate X_train and X_test and assing it to a variable X\n",
    "X = np.concatenate((X_train, y_train), axis=0)\n",
    "\n",
    "# Concatenate y_train and y_test and assing it to a variable y\n",
    "y = np.concatenate((X_test, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZObXVorUxoGK"
   },
   "source": [
    "###Padding\n",
    "\n",
    "Since all reviews are at different lengths, we'll use padding to make all of them same length.\n",
    "\n",
    "ðŸ“Œ Use preprocessing.sequence.pad_sequences() function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8mlvy8xKu7-"
   },
   "outputs": [],
   "source": [
    "# Pad all reviews in the X dataset to the length maxlen=1024\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rZILMK5_-e4"
   },
   "source": [
    "### Splitting\n",
    "\n",
    "Now, split X and y into train, validation and test dataset and assign those to corresponding values.\n",
    "\n",
    "ðŸ“Œ You can use list slicing methods for this.\n",
    "\n",
    "ðŸ“Œ For this dataset, a 80-10-10 split corresponds to 40000 - 10000 - 10000 number of samples relatively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ru_A80XWPr05"
   },
   "outputs": [],
   "source": [
    "# Create the training datasets\n",
    "X_train = X[:40000]\n",
    "y_train = y[:40000]\n",
    "\n",
    "# Create the validation datasets\n",
    "X_val = X[40000:45000]\n",
    "y_val = y[40000:45000]\n",
    "\n",
    "# Create the test datasets\n",
    "X_test = X[45000:50000]\n",
    "y_test = X[45000:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4t0TWEuCs6q"
   },
   "source": [
    "To check if that worked out, print the number of samples in each dataset again.\n",
    "\n",
    "ðŸ“Œ Use f-strings for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhRLn4stTA4d"
   },
   "outputs": [],
   "source": [
    "# Print the number of samples\n",
    "print(\"X_train:\",len(X_train))\n",
    "print(\"y_train:\",len(y_train))\n",
    "print(\"X_val:\",len(X_val))\n",
    "print(\"y_val:\",len(y_val))\n",
    "print(\"X_test:\",len(X_test))\n",
    "print(\"y_test:\",len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDCMa-o8ESLy"
   },
   "source": [
    "## Constructing the neural network\n",
    "\n",
    "That was it for the preprocessing of the data! \n",
    "\n",
    "Now we can create our model. First, we start by creating a model object using the Sequential API of Keras.\n",
    "\n",
    "ðŸ“Œ Use tf.keras.Sequential() to create a model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lodLU07jdzm"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lh7_MzgFhIf"
   },
   "source": [
    "### Embedding Layer\n",
    "\n",
    "For the first layer, we add an embedding layer.\n",
    "\n",
    "ðŸ“Œ Use tf.keras.layers.Embedding() for the embedding layer.\n",
    "\n",
    "ðŸ“Œ Use .add() method of the object to add the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41CLMa1Epasa"
   },
   "outputs": [],
   "source": [
    "# Add an embedding layer and a dropout\n",
    "model.add(tf.keras.layers.Embedding(input_dim=10000, output_dim=256))\n",
    "model.add(tf.keras.layers.Dropout(0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpeVhPpEG3u9"
   },
   "source": [
    "Then, we add a LSTM layer and a dense layer; each with a dropout.\n",
    "\n",
    "ðŸ“Œ Use tf.keras.layers.LSTM() and tf.keras.layers.Dense() to create the layers.\n",
    "\n",
    "ðŸ“Œ Use .add() method of the object to add the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntaW1KWrpngU"
   },
   "outputs": [],
   "source": [
    "# Add a LSTM layer with dropout\n",
    "model.add(tf.keras.layers.LSTM(256))\n",
    "model.add(tf.keras.layers.Dropout(0.7))\n",
    "\n",
    "# Add a Dense layer with dropout\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTWRJxTGHhaI"
   },
   "source": [
    "### Output layer\n",
    "\n",
    "As the last part of our neural network, we add the output layer. The number of nodes will be one since we are making binary classification. We'll use the sigmoid activation function in the output layer.\n",
    "\n",
    "ðŸ“Œ Use tf.keras.layers.Dense() to create the layer.\n",
    "\n",
    "ðŸ“Œ Use .add() method of the object to add the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ufBdJmBs_T-"
   },
   "outputs": [],
   "source": [
    "# Add the output layer\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7EI9LX1I522"
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "Now we have the structure of our model. To configure the model for training, we'll use the *.compile()* method. Inside the compile method, we have to define the following:\n",
    "*   \"Adam\" for optimizer\n",
    "*   \"Binary Crossentropy\" for the loss function\n",
    "\n",
    "\n",
    "ðŸ“Œ Construct the model with the .compile() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkDRiJNW_Dbu"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpcO1HLZJZtZ"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "It's time to train the model. We'll give the X_train and y_train datasets as the first two arguments. These will be used for training. And with the *validation_data* parameter, we'll give the X_val and y_val as a tuple.\n",
    "\n",
    "ðŸ“Œ Use .fit() method of the model object for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoTfLMTt4RQ1"
   },
   "outputs": [],
   "source": [
    "# Train the model for 5 epochs\n",
    "results = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEx98AYLJwhl"
   },
   "source": [
    "### Visualize the results\n",
    "\n",
    "After the model is trained, we can create a graph to visualize the change of loss over time. Results are held in:\n",
    "* results.history[\"loss\"]\n",
    "* results.history[\"val_loss\"]\n",
    "\n",
    "ðŸ“Œ Use plt.show() to display the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDw7KpHct81z"
   },
   "outputs": [],
   "source": [
    "# Plot the the training loss\n",
    "plt.plot(results.history['loss'], label='Train')\n",
    "\n",
    "# Plot the the validation loss\n",
    "plt.plot(results.history['val_loss'], label='Validation')\n",
    "\n",
    "# Name the x and y axises\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# Put legend table\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4f-9V6pKHfE"
   },
   "source": [
    "Now, do the same thing for accuracy.\n",
    "\n",
    "ðŸ“Œ Accuracy scores can be found in:\n",
    "* results.history[\"accuracy\"]\n",
    "* results.history[\"val_accuracy\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LUeUQAn_CkD"
   },
   "outputs": [],
   "source": [
    "# Plot the the training accuracy\n",
    "plt.plot(results.history['accuracy'])\n",
    "\n",
    "# Plot the the validation accuracy\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "\n",
    "# Name the x and y axises\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "# Put legend table\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnz14s_zKSq8"
   },
   "source": [
    "## Performance evaluation\n",
    "\n",
    "Let's use the test dataset that we created to evaluate the performance of the model.\n",
    "\n",
    "ðŸ“Œ Use test_on_batch() method with test dataset as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grHvXCZY_JVT"
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOJH4551KWWe"
   },
   "source": [
    "### Try a prediction\n",
    "\n",
    "Next, we take a sample and make a prediction on it.\n",
    "\n",
    "ðŸ“Œ Reshape the review to (1, 1024).\n",
    "\n",
    "ðŸ“Œ Use the .prediction() method of the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vda8VhZh_LiK"
   },
   "outputs": [],
   "source": [
    "# Make prediction on the reshaped sample\n",
    "prediction_result = model.predict(X_test[789].reshape(1,1024))\n",
    "print(\"Label: {}\".format(y_test[789])|\"Prediction: {}\".format(prediction_result))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Guided_Project_3.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
